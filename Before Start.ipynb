{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 구성 환경을 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런\n",
    "- 파이썬을 기반으로 한 머신러닝 라이브러리\n",
    "- 사이킷런 데이터셋 : \n",
    "    Toy Dataset : boston, iris, diabetes, digits, linnerud, sine, breast cancer 등 7가지 데이터셋.\n",
    "                  데이터셋이 비교적 작다.\n",
    "    Real world dataset : olivetti faces, 20newsgroups, covtype, california housing 등 현실 세계의 문제들과 \n",
    "                         관련한 9가지 데이터셋을 제공한다. \n",
    "                         \n",
    "** 이번 실습에서는 Toy Dataset 에서 제공하는 iris 데이터셋을 이용하여 붓꽃의 4가지 특성을 이용한 종류 분류를 시행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#사이킷 런에서 iris 데이터셋을 불러와 iris 변수에 저장\n",
    "iris = load_iris()\n",
    "\n",
    "#iris 데이터셋에서 확인 가능한 데이터의 형태는 (150개, 4쌍) 이다.\n",
    "iris_data = iris.data\n",
    "print(iris_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 (150, 4)는 아이리스 데이터의 형태를 말하는데, 이는 (150개의 데이터, 4쌍)을 의미한다.\n",
    "그럼 4쌍의 데이터는 각각 무엇을 의미하는 것일까? \n",
    "\n",
    "공식 가이드 문서를 살펴보면, 이 데이터 4쌍이 각각 어떤 정보를 의미하는 지 알 수 있다.\n",
    "![아이리스 구성](./PostingPic/iris구성.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 아래와 같이 iris[0]를 출력해보면 이 개별 값들을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_data[0] [5.1 3.5 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"iris_data[0]\", iris_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 마찬가지로, iris의 전체적인 데이터 구성을 확인해보자\n",
    "\n",
    "![iris 전체 데이터 구성](./PostingPic/iris_returns.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#150개의 아이리스 사진에 대한 라벨링 클래스\n",
    "iris_label = iris.target\n",
    "print(iris_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피쳐 네임즈 :\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "타겟 네임즈 : \n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "파일 네임 :\n",
      "/home/ssac23/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/datasets/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "#피쳐 네임즈 인쇄\n",
    "print(\"피쳐 네임즈 :\")\n",
    "print(iris.feature_names)\n",
    "\n",
    "\n",
    "#타겟 네임즈 인쇄\n",
    "print(\"\\n타겟 네임즈 : \")\n",
    "print(iris.target_names)\n",
    "\n",
    "#DESCR //데이터 셋에 대한 설명이 들어가있고, 인쇄 시 너무 길어져 주석처리했음.\n",
    "#print(iris.DESCR)\n",
    "\n",
    "#파일네임\n",
    "print(\"\\n파일 네임 :\")\n",
    "print(iris.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 판다스\n",
    "    - 파이썬에서, 표 형태로 이루어진 2차원 배열을 다루는 데 많이 쓰이는 도구 \n",
    "    - 판다스를 활용하여 데이터 셋을 표로 만들어본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     labels  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#pandas를 활용하여 dataFrame을 작성한다.\n",
    "iris_data_frame = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "iris_data_frame[\"labels\"] = iris.target\n",
    "\n",
    "iris_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.데이터를 준비하자 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 주어진 150개의 데이터셋을 트레이닝셋과 데이터셋으로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data 개수 : 120 X_test 개수 :  30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#테스트셋을 0.2의 비율로 나눈다. 랜덤 세트는 7로 고정한다.(다른 연구자들과 공유할 때 특정한 랜덤 셋이 필요하기 때문)\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=7)\n",
    "\n",
    "print('X_data 개수 :' , len(x_train), 'X_test 개수 : ' , len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 1, 0, 2, 1, 0, 0, 0, 0, 2, 2, 1, 2, 2, 1, 0, 1, 1, 2, 0, 0, 0,\n",
       "        2, 0, 2, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 2, 0, 0, 2, 2, 0, 2, 0,\n",
       "        1, 2, 1, 0, 1, 0, 2, 2, 1, 0, 0, 1, 2, 0, 2, 2, 1, 0, 1, 0, 2, 2,\n",
       "        0, 0, 2, 1, 2, 2, 1, 0, 0, 2, 0, 0, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1,\n",
       "        1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 2, 1, 2,\n",
       "        0, 2, 1, 1, 0, 2, 1, 2, 1, 0]),\n",
       " array([2, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 2, 2, 0, 0, 1, 2,\n",
       "        1, 2, 2, 2, 1, 1, 2, 2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y는 랜덤한 세트로 준비되었다.\n",
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.모델을 설계하고 학습시키자\n",
    "### 3-2. 어떻게 모델을 설계해야 할까?\n",
    "\n",
    "![머신러닝의 유형](./PostingPic/머신러닝_유형.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[이미지 출처는 여기](https://bestpractice80.tistory.com/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 위와 같이 머신러닝은 크게 '지도학습' 과 '비지도 학습' 의 분류로 나뉘어진다. \n",
    "    - 이 때, 우리는 꽃잎과 꽃받침의 길이에 따른 붓꽃 종류의 '문제' 와 '정답' 을 알려주고, 정답을 알려주지 않은 문제로 품종을 올바르게 맞힐 수 있는지를 테스트하는 것이므로 '지도학습' 을 할 수 있는 머신러닝을 설계해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 또한, __지도학습__ 중에서도 우리는 '분류', '회귀' 중 __분류__ 작업을 수행할 수 있는 머신러닝 모델을 설계해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3-3. 머신러닝의 다양한 모델들__ (중요!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision Tree(의사결정나무)\n",
    "\n",
    "![의사결정나무](./PostingPic/decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "루트 노드에서 각각의 조건에 따라 분기하고, 이를 따라 최종 결정을 내리게 된다. (분류, 예측 모두 가능)   \n",
    "\n",
    "    - 분류 나무는 각각 영역의 순도가 증가하고, 불순도가 감소하는 방향으로 학습을 진행한다.(이 때 엔트로피 감소 방향으로 결정)\n",
    "   - __재귀적 분기__ 와 __가지치기__ 과정을 거쳐서 학습한다.\n",
    "   - __재귀적 분기__ : 모든 경우의 수 가운데 정보획득이 가장 큰 변수와 지점을 택해 분기, 분기, 분기한다.\n",
    "   - __가지치기__ : 모든 터미널의 순도가 100%인 풀 트리(full tree)에서 과적합이 일어나지 않도록 하기 위해, 검증 데이터에 대한 오분류율이 증가하는 시점에서 가지치기를 수행하여 \"분기를 합친다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> 하지만, 이러한 의사결정 나무는 결정 경계가 데이터 축에 대해 수직이어서 특정 데이터에만 잘 작동할 수 있다.\n",
    "    이를 극복하기 위해 __Random Forest__ 기법이 제안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 의사결정나무 모델을 사이킷런 패키지에서 가져와 학습시킨다.\n",
    "decision_tree = DecisionTreeClassifier(random_state=32)\n",
    "decision_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predic : [2 1 0 1 2 0 1 1 0 1 2 1 0 2 0 2 2 2 0 0 1 2 1 1 2 2 1 1 2 2]\n",
      "y_test : [2 1 0 1 2 0 1 1 0 1 1 1 0 2 0 1 2 2 0 0 1 2 1 2 2 2 1 1 2 2]\n",
      "accuracy : 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 의사결정나무 모델의 학습 성과를 테스트 데이터로 예측한다.\n",
    "# 수동적으로 y_predict와 y_test를 출력 비교하여 정확도를 확인할 수 있다.\n",
    "y_predict = decision_tree.predict(x_test)\n",
    "\n",
    "print('y_predic :', y_predict)\n",
    "print('y_test :' , y_test)\n",
    "\n",
    "# 정확도를 측정하는 함수 accuracy_score 를 가지고 이를 확인할 수 있다.\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print('accuracy :', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이걸 __표로 확인해보자!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       0.83      0.91      0.87        11\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.91      0.91      0.91        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest(랜덤 포레스트)\n",
    "\n",
    "   - 의사결정 나무의 단점을 극복하기 위한 방법으로, 각 데이터에 대한 의사결정나무를 여러 개 정하고 그 의사결정나무의 결론을 다수결로 모아   하나의 결정을 내린다.(이를 __'앙상블 기법'__ 이라고 한다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![랜덤 포레스트 기법](./PostingPic/random_forest.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __랜덤 포레스트 기법의 순서__\n",
    " > 1. 결정을 내리기 위해 고려하고 싶은 25개의 요소가 있다고 할 때\n",
    " > 2. 각각의 의사결정트리가 취하는 요소를 5개로 정하면,\n",
    " > 3. 랜덤 포레스트를 이루는 여러 개의 의사결정트리를 만든다.\n",
    " \n",
    " > 4. 각각의 의사결정 트리는 25개 중 랜덤으로 5개의 요소를 취하여 트리별로 나름의 결정을 내린다.\n",
    " > 5. 랜덤 포레스트는 이 트리들의 의견을 다수결로 판단하여\n",
    " > 6. 랜덤 포레스트 하나의 의견이 나오게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   __랜덤포레스트 기법의 적용__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=25)\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=25)\n",
    "random_forest.fit(x_train, y_train)\n",
    "y_predict = random_forest.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 랜덤포레스트 기법의 학습 후, 정확도를 측정하니 0.93이 측정되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SVM 모델 (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 학습 데이터를 비선형 매핑을 통해 고차원으로 변환하여, 이 고차원에서 초평면을 최적으로 분리하는 선형분리를 찾는다.\n",
    "    (2차원에서는 XOR 문제처럼 선형으로 분리될 수 없는 것처럼 보이는 것들도, 차원을 높이면 분리되는 지점이 보인다.)\n",
    "    \n",
    "   ![SVM모델](./PostingPic/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   __SVM 기법의 적용__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.88      0.88      0.88        30\n",
      "weighted avg       0.87      0.87      0.87        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "# 데이터 셋을 나눈다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "# SVM 모델\n",
    "svm_model=svm.SVC()\n",
    "svm_model.fit(x_train, y_train)\n",
    "y_predict=svm_model.predict(x_test)\n",
    "\n",
    "# SVM 모델 평가\n",
    "svm_report=classification_report(y_test, y_predict)\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * SVM 기법의 학습 후, 정확도를 측정하니 0.87이 측정되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.SGD 모델(Stochastic Gradient Descent Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 일반적인 경사 하강법이 cost가 많이 든다(계산량이 매우 많이 듬)는 단점을 보완하기 위해,    \n",
    "    랜덤하게 추출한 일부 데이터에 대해 가중치를 조절하는 방법이다.   \n",
    "    코스트가 줄어듦으로 인해 속도는 개선되지만, 정확도는 경사 하강법에 비해 떨어질 수 있다.\n",
    "   \n",
    "   ![확률적 경사 하강법](./PostingPic/sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - SGD 모델의 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80         7\n",
      "           1       0.70      0.58      0.64        12\n",
      "           2       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.73        30\n",
      "   macro avg       0.73      0.75      0.74        30\n",
      "weighted avg       0.73      0.73      0.73        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 데이터 셋을 나눈다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=7)\n",
    "\n",
    "# SGD 모델\n",
    "sgd_model=SGDClassifier()\n",
    "\n",
    "sgd_model.fit(x_train, y_train)\n",
    "y_predict=sgd_model.predict(x_test)\n",
    "\n",
    "# SGD 모델 평가\n",
    "sgd_report=classification_report(y_test, y_predict)\n",
    "print(sgd_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * SGD 기법의 학습 후, 정확도를 측정하니 0.73이 측정되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 로지스틱 회귀 모델(Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 로지스틱 회귀 모델은 선형 회귀모델의 <--> 무한대 범주를, 시그노이드를 취하여 0에서 1사이의 확률로 변환할 수 있게끔 하는 모델이다.\n",
    "   \n",
    "   ![로지스틱 회귀 모델](./PostingPic/logisticRegression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 로지스틱 회귀 모델의 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.83      0.83      0.83        12\n",
      "           2       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.88      0.88      0.88        30\n",
      "weighted avg       0.87      0.87      0.87        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac23/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_model.fit(x_train, y_train)\n",
    "y_predict = logistic_model.predict(x_test)\n",
    "\n",
    "logistic_report = classification_report(y_test, y_predict)\n",
    "print(logistic_report)\n",
    "\n",
    "\n",
    "# 아래에 나오는 저 오류는 뭘까.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * SGD 기법의 학습 후, 정확도를 측정하니 0.87이 측정되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델의 정확도를 측정하는 다른 방법\n",
    "## 4-1. 정확도도 안 맞을 수 있다?\n",
    "\n",
    "    * 편향된 데이터가 있을 때, 정확도는 모델의 성능을 평가하는 지표로 기능하지 못한다.\n",
    "    ex) 0에서 9까지의 손글씨 분류기에서, 3 이외의 다른 데이터를 '틀린 데이터' 로 만들고 모델을 학습시킬 경우,    \n",
    "    3 데이터의 비율:3을 제외한 다른 데이터의 비율=1:9 가 되어 편중되고,   \n",
    "    모델은 편중된 데이터에 따라 '정확한 정답을 맞히기' 보다는 '틀리지 않기' 에 집중하게 된다.\n",
    "    모델이 실제로 '정답을 정확하게 맞추었는가' 와 상관없이 '정답을 틀리는 것을 피하기' 에 집중함으로써,   \n",
    "    정확도는 0.9 이상을 기록하지만 모델을 '신뢰할 수 있는가' 는 장담할 수 없게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. 오차행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
